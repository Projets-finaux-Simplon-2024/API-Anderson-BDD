{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fichier de test et d'installation du modele Solon-embeddings-large-0.1\n",
    "lien : https://huggingface.co/OrdalieTech/Solon-embeddings-large-0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Imports et initialisations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bucket 'mlflow' existe d√©j√†.\n",
      "MinIO URL: http://localhost:9000\n",
      "AWS Access Key: minioadmin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='s3://mlflow/artifacts/1', creation_time=1723348421747, experiment_id='1', last_update_time=1723348421747, lifecycle_stage='active', name='Solon-embeddings', tags={}>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import mlflow\n",
    "import mlflow.pytorch\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "import logging\n",
    "import boto3 # <-------------------- D√©pendance requise par MLFlow pour pouvoir utiliser un service de stockage S3 (Minio)\n",
    "import mlflow.pyfunc\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from dotenv import load_dotenv\n",
    "from minio import Minio, S3Error\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from mlflow.models import infer_signature, validate_serving_input, convert_input_example_to_serving_input\n",
    "from mlflow.tracking import MlflowClient\n",
    "\n",
    "# Demande peut-√™tre de la configuration apr√®s un pull\n",
    "MINIO_URL=\"localhost:9000\"\n",
    "MINIO_ACCESS_KEY=\"minioadmin\"\n",
    "MINIO_SECRET_KEY=\"minioadmin\"\n",
    "MLFLOW_TRACKING_URI=\"http://localhost:5000\"\n",
    "MLFLOW_DEFAULT_ARTIFACT_ROOT=\"s3://mlflow/artifacts\"\n",
    "MLFLOW_S3_ENDPOINT_URL=\"http://localhost:9000\"\n",
    "AWS_ACCESS_KEY_ID=\"minioadmin\"\n",
    "AWS_SECRET_ACCESS_KEY=\"minioadmin\"\n",
    "\n",
    "# Initialiser le client MinIO\n",
    "minio_client = Minio(\n",
    "    MINIO_URL,\n",
    "    access_key=MINIO_ACCESS_KEY,\n",
    "    secret_key=MINIO_SECRET_KEY,\n",
    "    secure=False  # <-------------------- mettre √† True pour utiliser HTTPS\n",
    ")\n",
    "\n",
    "# Nom du bucket (dossier) pour les artefacts MLflow\n",
    "bucket_name = 'mlflow'\n",
    "\n",
    "# Cr√©er le bucket s'il n'existe pas\n",
    "try:\n",
    "    if not minio_client.bucket_exists(bucket_name):\n",
    "        minio_client.make_bucket(bucket_name)\n",
    "        print(f\"Bucket '{bucket_name}' cr√©√© avec succ√®s.\")\n",
    "    else:\n",
    "        print(f\"Bucket '{bucket_name}' existe d√©j√†.\")\n",
    "except S3Error as err:\n",
    "    print(f\"Erreur lors de la cr√©ation du bucket: {err}\")\n",
    "\n",
    "# V√©rifiez les variables d'environnement pour MinIO\n",
    "print(\"MinIO URL:\", MLFLOW_S3_ENDPOINT_URL)\n",
    "print(\"AWS Access Key:\", AWS_ACCESS_KEY_ID)\n",
    "\n",
    "\n",
    "# D√©finir l'URI de suivi MLflow pour pointer vers votre instance locale\n",
    "mlflow.set_tracking_uri(MLFLOW_TRACKING_URI)\n",
    "mlflow.set_experiment(\"Solon-embeddings\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## T√©l√©chargement du mod√®le \n",
    "T√©l√©charge et cr√©er un dossier \"models\" dans \"install_models\" avec \"special_tokens_map.json\" | \"tokenizer_config.json\" | \"tokenizer.json\"\n",
    "```\n",
    "app\n",
    "|_install_models\n",
    "|__models\n",
    "|___special_tokens_map.json\n",
    "|___tokenizer_config.json\n",
    "|___tokenizer.json\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger le tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"OrdalieTech/Solon-embeddings-large-0.1\")\n",
    "\n",
    "# Charger le mod√®le\n",
    "model = AutoModel.from_pretrained(\"OrdalieTech/Solon-embeddings-large-0.1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Test du mod√®le avec la partie locale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.96402365  1.4223621   1.0408337  ...  0.1549829  -0.38752386\n",
      "   0.11577153]\n",
      " [ 0.36774197  0.13094549  0.96774954 ... -0.9510918   0.21017613\n",
      "   0.7587245 ]\n",
      " [ 0.0997332   1.0194489   0.86829764 ...  0.72202337  0.20158945\n",
      "   0.16800208]\n",
      " [ 0.9345613   0.41134351  0.66911495 ...  0.17017119  0.39119667\n",
      "   0.18250458]\n",
      " [ 1.1195613   1.6284131  -0.20209625 ... -1.39997     0.80081695\n",
      "   1.1151884 ]]\n"
     ]
    }
   ],
   "source": [
    "def extract_features(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    return outputs.last_hidden_state.mean(dim=1).numpy()\n",
    "\n",
    "text = [\"Il fait beau\", \"Il est beau\", \"Il va faire beau\", \"Il a fait beau\", \"C'est tr√®s beau\"]\n",
    "features = extract_features(text)\n",
    "print(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Encapsulation du mod√®le et customisation de predict\n",
    "Fait correspondre le mod√®le a une PyFunc car utilis√© predict pour un mod√®le sauvegarder sur mlflow avec pytorch pose probl√®me aux inputs qui ne correspondent pas, predict attend un float de base et le mod√®le lui a besoin d'un string."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enregistrement sur le server MLFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Registered model 'solon-embeddings-large-model' already exists. Creating a new version of this model...\n",
      "2024/08/12 17:49:36 INFO mlflow.store.model_registry.abstract_store: Waiting up to 300 seconds for model version to finish creation. Model name: solon-embeddings-large-model, version 2\n",
      "Created version '2' of model 'solon-embeddings-large-model'.\n",
      "2024/08/12 17:49:36 WARNING mlflow.models.model: Input example should be provided to infer model signature if the model signature is not provided when logging the model.\n",
      "2024/08/12 17:49:36 INFO mlflow.tracking._tracking_service.client: üèÉ View run nimble-fawn-832 at: http://localhost:5000/#/experiments/1/runs/49b763db850845ac8a557162e01db1a6.\n",
      "2024/08/12 17:49:36 INFO mlflow.tracking._tracking_service.client: üß™ View experiment at: http://localhost:5000/#/experiments/1.\n"
     ]
    }
   ],
   "source": [
    "class PyTorchModelWrapper(mlflow.pyfunc.PythonModel):\n",
    "    def load_context(self, context):\n",
    "        # Charger le mod√®le PyTorch et le tokenizer\n",
    "        self.model = model  # Remplacez par le chemin si vous souhaitez le charger depuis un fichier\n",
    "        self.tokenizer = tokenizer  # Idem pour le tokenizer\n",
    "\n",
    "    def predict(self, context, model_input):\n",
    "        # Tokenization des inputs\n",
    "        inputs = self.tokenizer(model_input, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "        \n",
    "        # D√©sactiver le gradient pour l'inf√©rence\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs)\n",
    "        \n",
    "        # Retourner la moyenne des embeddings (ou toute autre op√©ration que vous souhaitez effectuer)\n",
    "        return outputs.last_hidden_state.mean(dim=1).numpy()\n",
    "\n",
    "# Enregistrement du mod√®le PyFunc encapsul√© sans exemple d'entr√©e\n",
    "with mlflow.start_run():\n",
    "    mlflow.pyfunc.log_model(\n",
    "        artifact_path=\"solon-embeddings-large-model\",\n",
    "        python_model=PyTorchModelWrapper(),\n",
    "        conda_env=mlflow.pytorch.get_default_conda_env(),\n",
    "        registered_model_name=\"solon-embeddings-large-model\",\n",
    "    )\n",
    "    mlflow.log_param(\"model_name\", \"OrdalieTech/Solon-embeddings-large-0.1\")\n",
    "    mlflow.log_param(\"source\", \"Script d'installation Solon-embeddings-large-0.1.ipynb\")\n",
    "    mlflow.log_param(\"model_version\", \"none\")\n",
    "    mlflow.log_metric(\"mean_cos_similarity\", 0)\n",
    "    mlflow.log_metric(\"cos_similarity_top_1\", 0)\n",
    "    mlflow.log_metric(\"cos_similarity_top_2\", 0)\n",
    "    mlflow.log_metric(\"cos_similarity_top_3\", 0)\n",
    "    mlflow.log_metric(\"cos_similarity_top_4\", 0)\n",
    "    mlflow.log_metric(\"cos_similarity_top_5\", 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installation en local ou sur un stockage\n",
    "ATTENTION installe le mod√®le ou sur un stockage type blob etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mlflow.pyfunc.save_model(\\n    path=\"models/solon-embeddings-large-model\",\\n    python_model=PyTorchModelWrapper(),\\n    conda_env=mlflow.pytorch.get_default_conda_env(),\\n    signature=signature,\\n    input_example=input_example\\n)'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"mlflow.pyfunc.save_model(\n",
    "    path=\"models/solon-embeddings-large-model\",\n",
    "    python_model=PyTorchModelWrapper(),\n",
    "    conda_env=mlflow.pytorch.get_default_conda_env(),\n",
    "    signature=signature,\n",
    "    input_example=input_example\n",
    ")\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Test du mod√®le via MLFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### R√©cup√©ration du dernier mod√®le"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La derni√®re version du mod√®le solon-embeddings-large-model est : 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jejel\\AppData\\Local\\Temp\\ipykernel_780592\\1989203347.py:8: FutureWarning: ``mlflow.tracking.client.MlflowClient.get_latest_versions`` is deprecated since 2.9.0. Model registry stages will be removed in a future major release. To learn more about the deprecation of model registry stages, see our migration guide here: https://mlflow.org/docs/latest/model-registry.html#migrating-from-stages\n",
      "  model_versions = client.get_latest_versions(model_name, stages=[\"None\", \"Staging\", \"Production\"])\n"
     ]
    }
   ],
   "source": [
    "# Nom du mod√®le enregistr√©\n",
    "model_name = \"solon-embeddings-large-model\"\n",
    "\n",
    "# Cr√©er une instance de MlflowClient\n",
    "client = MlflowClient()\n",
    "\n",
    "# R√©cup√©rer toutes les versions du mod√®le\n",
    "model_versions = client.get_latest_versions(model_name, stages=[\"None\", \"Staging\", \"Production\"])\n",
    "\n",
    "# Filtrer la derni√®re version du mod√®le en fonction de l'ordre de version\n",
    "latest_version = max([int(version.version) for version in model_versions])\n",
    "\n",
    "print(f\"La derni√®re version du mod√®le {model_name} est : {latest_version}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test du dernier modele enregistrer via MLFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1de993251b014be39bf7c693482a2238",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/08/12 17:49:44 INFO mlflow.tracking._tracking_service.client: üèÉ View run serious-moth-891 at: http://localhost:5000/#/experiments/1/runs/2f48452a4dbd418e8040da14c2dd6b71.\n",
      "2024/08/12 17:49:44 INFO mlflow.tracking._tracking_service.client: üß™ View experiment at: http://localhost:5000/#/experiments/1.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Charger le mod√®le depuis MLflow\n",
    "model_uri = f\"models:/solon-embeddings-large-model/{latest_version}\"  # Pour l'utilisation du mod√®le d'un run c'est run:/<id_run>/ \n",
    "loaded_model = mlflow.pyfunc.load_model(model_uri)\n",
    "\n",
    "# Exemple de question et de r√©ponses\n",
    "question = [\"Il fait beau\"]\n",
    "responses = [\"Il fait beau\", \"Il est beau\", \"Il va faire beau\", \"Il a fait beau\", \"C'est tr√®s beau\"]\n",
    "response_1 = [\"Il fait beau\"]\n",
    "response_2 = [\"Il est beau\"]\n",
    "response_3 = [\"Il va faire beau\"]\n",
    "response_4 = [\"Il a fait beau\"]\n",
    "response_5 = [\"C'est tr√®s beau\"]\n",
    "\n",
    "# Extraire les embeddings\n",
    "question_embedding = loaded_model.predict(question)\n",
    "responses_embeddings = loaded_model.predict(responses)\n",
    "response_1_embedding = loaded_model.predict(response_1)\n",
    "response_2_embedding = loaded_model.predict(response_2)\n",
    "response_3_embedding = loaded_model.predict(response_3)\n",
    "response_4_embedding = loaded_model.predict(response_4)\n",
    "response_5_embedding = loaded_model.predict(response_5)\n",
    "\n",
    "# Calculer la similarit√© cosinus entre la question et les r√©ponses\n",
    "cos_similarities = cosine_similarity(question_embedding, responses_embeddings)\n",
    "\n",
    "# Exemple de valeur moyenne de la similarit√© cosinus (pour une paire)\n",
    "mean_cos_similarity = np.mean(cos_similarities)\n",
    "\n",
    "cos_similaritie_1 = cosine_similarity(question_embedding, response_1_embedding)\n",
    "cos_similaritie_2 = cosine_similarity(question_embedding, response_2_embedding)\n",
    "cos_similaritie_3 = cosine_similarity(question_embedding, response_3_embedding)\n",
    "cos_similaritie_4 = cosine_similarity(question_embedding, response_4_embedding)\n",
    "cos_similaritie_5 = cosine_similarity(question_embedding, response_5_embedding)\n",
    "\n",
    "with mlflow.start_run() as run:\n",
    "    # Enregistrer les param√®tres et les m√©triques\n",
    "    mlflow.log_param(\"model_name\", \"OrdalieTech/Solon-embeddings-large-0.1\")\n",
    "    mlflow.log_param(\"source\", \"Script de test Solon-embeddings-large-0.1.ipynb\")\n",
    "    mlflow.log_param(\"model_version\", f\"solon-embeddings-large-model v{latest_version}\")\n",
    "    mlflow.log_metric(\"mean_cos_similarity\", mean_cos_similarity)\n",
    "    mlflow.log_metric(\"cos_similarity_top_1\", cos_similaritie_1)\n",
    "    mlflow.log_metric(\"cos_similarity_top_2\", cos_similaritie_2)\n",
    "    mlflow.log_metric(\"cos_similarity_top_3\", cos_similaritie_3)\n",
    "    mlflow.log_metric(\"cos_similarity_top_4\", cos_similaritie_4)\n",
    "    mlflow.log_metric(\"cos_similarity_top_5\", cos_similaritie_5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_api_anderson",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
