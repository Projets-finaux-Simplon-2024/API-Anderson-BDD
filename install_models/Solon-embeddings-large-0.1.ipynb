{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fichier de test et d'installation du modele Solon-embeddings-large-0.1\n",
    "lien : https://huggingface.co/OrdalieTech/Solon-embeddings-large-0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Imports et initialisations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bucket 'mlflow' existe déjà.\n",
      "MinIO URL: http://localhost:9000\n",
      "AWS Access Key: minioadmin\n",
      "Aucune table trouvée dans la base de données. Exécution des migrations Alembic...\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "import mlflow.pytorch\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "import logging\n",
    "import boto3 # <-------------------- Dépendance requise par MLFlow pour pouvoir utiliser un service de stockage S3 (Minio)\n",
    "import mlflow.pyfunc\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from dotenv import load_dotenv\n",
    "from minio import Minio, S3Error\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from mlflow.models import infer_signature, validate_serving_input, convert_input_example_to_serving_input\n",
    "from mlflow.tracking import MlflowClient\n",
    "from sqlalchemy import create_engine, inspect\n",
    "from alembic.config import Config\n",
    "from alembic import command\n",
    "\n",
    "# Demande peut-être de la configuration après un pull\n",
    "MINIO_URL=\"localhost:9000\"\n",
    "MINIO_ACCESS_KEY=\"minioadmin\"\n",
    "MINIO_SECRET_KEY=\"minioadmin\"\n",
    "MLFLOW_TRACKING_URI=\"http://localhost:5000\"\n",
    "MLFLOW_DEFAULT_ARTIFACT_ROOT=\"s3://mlflow/artifacts\"\n",
    "MLFLOW_S3_ENDPOINT_URL=\"http://localhost:9000\"\n",
    "AWS_ACCESS_KEY_ID=\"minioadmin\"\n",
    "AWS_SECRET_ACCESS_KEY=\"minioadmin\"\n",
    "DATABASE_URL=\"postgresql://admin:admin@localhost/anderson\"\n",
    "\n",
    "# Initialiser le client MinIO\n",
    "minio_client = Minio(\n",
    "    MINIO_URL,\n",
    "    access_key=MINIO_ACCESS_KEY,\n",
    "    secret_key=MINIO_SECRET_KEY,\n",
    "    secure=False  # <-------------------- mettre à True pour utiliser HTTPS\n",
    ")\n",
    "\n",
    "# Nom du bucket (dossier) pour les artefacts MLflow\n",
    "bucket_name = 'mlflow'\n",
    "\n",
    "# Créer le bucket s'il n'existe pas\n",
    "try:\n",
    "    if not minio_client.bucket_exists(bucket_name):\n",
    "        minio_client.make_bucket(bucket_name)\n",
    "        print(f\"Bucket '{bucket_name}' créé avec succès.\")\n",
    "    else:\n",
    "        print(f\"Bucket '{bucket_name}' existe déjà.\")\n",
    "except S3Error as err:\n",
    "    print(f\"Erreur lors de la création du bucket: {err}\")\n",
    "\n",
    "# Vérifiez les variables d'environnement pour MinIO\n",
    "print(\"MinIO URL:\", MLFLOW_S3_ENDPOINT_URL)\n",
    "print(\"AWS Access Key:\", AWS_ACCESS_KEY_ID)\n",
    "\n",
    "\n",
    "# Définir l'URI de suivi MLflow pour pointer vers votre instance locale\n",
    "mlflow.set_tracking_uri(MLFLOW_TRACKING_URI)\n",
    "mlflow.set_experiment(\"Solon-embeddings\")\n",
    "\n",
    "# Initialiser le moteur SQLAlchemy\n",
    "engine = create_engine(DATABASE_URL)\n",
    "inspector = inspect(engine)\n",
    "tables = inspector.get_table_names()\n",
    "\n",
    "if not tables:\n",
    "    print(\"Aucune table trouvée dans la base de données. Exécution des migrations Alembic...\")\n",
    "\n",
    "    # Changez de répertoire à l'aide de subprocess\n",
    "    subprocess.run(\"cd .. && alembic upgrade head\", shell=True, check=True)\n",
    "else:\n",
    "    print(f\"Tables trouvées dans la base de données : {tables}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Téléchargement du modèle \n",
    "Télécharge et créer un dossier \"models\" dans \"install_models\" avec \"special_tokens_map.json\" | \"tokenizer_config.json\" | \"tokenizer.json\"\n",
    "```\n",
    "app\n",
    "|_install_models\n",
    "|__models\n",
    "|___special_tokens_map.json\n",
    "|___tokenizer_config.json\n",
    "|___tokenizer.json\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger le tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"OrdalieTech/Solon-embeddings-large-0.1\")\n",
    "\n",
    "# Charger le modèle\n",
    "model = AutoModel.from_pretrained(\"OrdalieTech/Solon-embeddings-large-0.1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Test du modèle avec la partie locale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    return outputs.last_hidden_state.mean(dim=1).numpy()\n",
    "\n",
    "text = [\"Il fait beau\", \"Il est beau\", \"Il va faire beau\", \"Il a fait beau\", \"C'est très beau\"]\n",
    "features = extract_features(text)\n",
    "print(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Encapsulation du modèle et customisation de predict\n",
    "Fait correspondre le modèle a une PyFunc car utilisé predict pour un modèle sauvegarder sur mlflow avec pytorch pose problème aux inputs qui ne correspondent pas, predict attend un float de base et le modèle lui a besoin d'un string."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enregistrement sur le server MLFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PyTorchModelWrapper(mlflow.pyfunc.PythonModel):\n",
    "    def load_context(self, context):\n",
    "        # Charger le modèle PyTorch et le tokenizer\n",
    "        self.model = model  # Remplacez par le chemin si vous souhaitez le charger depuis un fichier\n",
    "        self.tokenizer = tokenizer  # Idem pour le tokenizer\n",
    "\n",
    "    def predict(self, context, model_input):\n",
    "        # Tokenization des inputs\n",
    "        inputs = self.tokenizer(model_input, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "        \n",
    "        # Désactiver le gradient pour l'inférence\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs)\n",
    "        \n",
    "        # Retourner la moyenne des embeddings (ou toute autre opération que vous souhaitez effectuer)\n",
    "        return outputs.last_hidden_state.mean(dim=1).numpy()\n",
    "\n",
    "# Enregistrement du modèle PyFunc encapsulé sans exemple d'entrée\n",
    "with mlflow.start_run():\n",
    "    mlflow.pyfunc.log_model(\n",
    "        artifact_path=\"solon-embeddings-large-model\",\n",
    "        python_model=PyTorchModelWrapper(),\n",
    "        conda_env=mlflow.pytorch.get_default_conda_env(),\n",
    "        registered_model_name=\"solon-embeddings-large-model\",\n",
    "    )\n",
    "    mlflow.log_param(\"model_name\", \"OrdalieTech/Solon-embeddings-large-0.1\")\n",
    "    mlflow.log_param(\"source\", \"Script d'installation Solon-embeddings-large-0.1.ipynb\")\n",
    "    mlflow.log_param(\"model_version\", \"none\")\n",
    "    mlflow.log_metric(\"mean_cos_similarity\", 0)\n",
    "    mlflow.log_metric(\"cos_similarity_top_1\", 0)\n",
    "    mlflow.log_metric(\"cos_similarity_top_2\", 0)\n",
    "    mlflow.log_metric(\"cos_similarity_top_3\", 0)\n",
    "    mlflow.log_metric(\"cos_similarity_top_4\", 0)\n",
    "    mlflow.log_metric(\"cos_similarity_top_5\", 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installation en local ou sur un stockage\n",
    "ATTENTION installe le modèle ou sur un stockage type blob etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"mlflow.pyfunc.save_model(\n",
    "    path=\"models/solon-embeddings-large-model\",\n",
    "    python_model=PyTorchModelWrapper(),\n",
    "    conda_env=mlflow.pytorch.get_default_conda_env(),\n",
    "    signature=signature,\n",
    "    input_example=input_example\n",
    ")\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Test du modèle via MLFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Récupération du dernier modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nom du modèle enregistré\n",
    "model_name = \"solon-embeddings-large-model\"\n",
    "\n",
    "# Créer une instance de MlflowClient\n",
    "client = MlflowClient()\n",
    "\n",
    "# Récupérer toutes les versions du modèle\n",
    "model_versions = client.get_latest_versions(model_name, stages=[\"None\", \"Staging\", \"Production\"])\n",
    "\n",
    "# Filtrer la dernière version du modèle en fonction de l'ordre de version\n",
    "latest_version = max([int(version.version) for version in model_versions])\n",
    "\n",
    "print(f\"La dernière version du modèle {model_name} est : {latest_version}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test du dernier modele enregistrer via MLFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Charger le modèle depuis MLflow\n",
    "model_uri = f\"models:/solon-embeddings-large-model/{latest_version}\"  # Pour l'utilisation du modèle d'un run c'est run:/<id_run>/ \n",
    "loaded_model = mlflow.pyfunc.load_model(model_uri)\n",
    "\n",
    "# Exemple de question et de réponses\n",
    "question = [\"Il fait beau\"]\n",
    "responses = [\"Il fait beau\", \"Il est beau\", \"Il va faire beau\", \"Il a fait beau\", \"C'est très beau\"]\n",
    "response_1 = [\"Il fait beau\"]\n",
    "response_2 = [\"Il est beau\"]\n",
    "response_3 = [\"Il va faire beau\"]\n",
    "response_4 = [\"Il a fait beau\"]\n",
    "response_5 = [\"C'est très beau\"]\n",
    "\n",
    "# Extraire les embeddings\n",
    "question_embedding = loaded_model.predict(question)\n",
    "responses_embeddings = loaded_model.predict(responses)\n",
    "response_1_embedding = loaded_model.predict(response_1)\n",
    "response_2_embedding = loaded_model.predict(response_2)\n",
    "response_3_embedding = loaded_model.predict(response_3)\n",
    "response_4_embedding = loaded_model.predict(response_4)\n",
    "response_5_embedding = loaded_model.predict(response_5)\n",
    "\n",
    "# Calculer la similarité cosinus entre la question et les réponses\n",
    "cos_similarities = cosine_similarity(question_embedding, responses_embeddings)\n",
    "\n",
    "# Exemple de valeur moyenne de la similarité cosinus (pour une paire)\n",
    "mean_cos_similarity = np.mean(cos_similarities)\n",
    "\n",
    "cos_similaritie_1 = cosine_similarity(question_embedding, response_1_embedding)\n",
    "cos_similaritie_2 = cosine_similarity(question_embedding, response_2_embedding)\n",
    "cos_similaritie_3 = cosine_similarity(question_embedding, response_3_embedding)\n",
    "cos_similaritie_4 = cosine_similarity(question_embedding, response_4_embedding)\n",
    "cos_similaritie_5 = cosine_similarity(question_embedding, response_5_embedding)\n",
    "\n",
    "with mlflow.start_run() as run:\n",
    "    # Enregistrer les paramètres et les métriques\n",
    "    mlflow.log_param(\"model_name\", \"OrdalieTech/Solon-embeddings-large-0.1\")\n",
    "    mlflow.log_param(\"source\", \"Script de test Solon-embeddings-large-0.1.ipynb\")\n",
    "    mlflow.log_param(\"model_version\", f\"solon-embeddings-large-model v{latest_version}\")\n",
    "    mlflow.log_metric(\"mean_cos_similarity\", mean_cos_similarity)\n",
    "    mlflow.log_metric(\"cos_similarity_top_1\", cos_similaritie_1)\n",
    "    mlflow.log_metric(\"cos_similarity_top_2\", cos_similaritie_2)\n",
    "    mlflow.log_metric(\"cos_similarity_top_3\", cos_similaritie_3)\n",
    "    mlflow.log_metric(\"cos_similarity_top_4\", cos_similaritie_4)\n",
    "    mlflow.log_metric(\"cos_similarity_top_5\", cos_similaritie_5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_api_anderson",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
